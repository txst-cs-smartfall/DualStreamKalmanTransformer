# Option 1 V2: Improved Shared-Weight Dual Stream IMU Configuration
# INCREASED CAPACITY: ~10-12K params (vs 2.5K in V1)
# Addresses underfitting by increasing embed_dim and num_layers

model: Models.imu_dual_stream_shared.DualStreamSharedIMU
dataset: smartfallmm

# Subjects for fall detection (young participants only)
subjects: [29,30,31,32,34,35,36,37,38,39,43,44,45,46,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63]

# Validation subjects
validation_subjects: [38, 44]
# Subjects with poor gyroscope data - permanently in training, never tested (consistency across all configs)
train_only_subjects: [29, 30, 32, 35, 39, 59]  # Subjects with poor gyro data or imbalanced class distribution

model_args:
  num_layers: 2          # INCREASED from 1 → 2 for better temporal modeling
  norm_first: True
  embed_dim: 32          # INCREASED from 16 → 32 for more capacity
  activation: relu
  imu_channels: 6        # 6 channels: ax, ay, az, gx, gy, gz
  acc_coords: 6          # Alias for backward compatibility
  num_classes: 1         # Binary fall detection
  imu_frames: 128        # Window size
  acc_frames: 128        # Alias for backward compatibility
  mocap_frames: 128      # For compatibility
  num_heads: 4           # INCREASED from 2 → 4 for richer attention
  dropout: 0.6           # REDUCED from 0.65 (more capacity = less aggressive dropout)

dataset_args:
  mode: 'sliding_window'
  max_length: 128
  task: 'fd'
  modalities: ['skeleton', 'accelerometer', 'gyroscope']
  age_group: ['young']
  sensors: ['watch']
  use_skeleton: False
  enable_filtering: False
  filter_cutoff: 5.5
  filter_fs: 25
  enable_normalization: False
  discard_mismatched_modalities: True
  length_sensitive_modalities: ['accelerometer', 'gyroscope']

# Training configuration
batch_size: 64
test_batch_size: 64
val_batch_size: 64
num_epoch: 120          # INCREASED from 80 (smaller models need more training)

# Dataloader
feeder: Feeder.Make_Dataset.UTD_mm

# Feeder arguments
train_feeder_args:
  batch_size: 64

val_feeder_args:
  batch_size: 64

test_feeder_args:
  batch_size: 64

# Random seed
seed: 2

# Optimizer configuration
optimizer: adamw
base_lr: 1e-3
weight_decay: 1e-3

# Notes:
# V2 IMPROVEMENTS:
# - 4x more embedding capacity (16→32)
# - 2x more transformer layers (1→2)
# - 2x more attention heads (2→4)
# - Expected params: ~10-12K (vs 2.5K in V1, still only 40% of baseline)
# - Should significantly reduce underfitting
