# Accelerometer + Orientation from Madgwick Sensor Fusion
# 7 channels: [smv, ax, ay, az, roll, pitch, yaw]
#
# Uses Madgwick AHRS algorithm to fuse accelerometer and gyroscope into
# orientation angles (roll, pitch, yaw). This corrects gyroscope drift using
# accelerometer gravity reference, providing orientation-aware features.
#
# Key advantages:
# - Orientation features capture device pose relative to gravity
# - Madgwick filter corrects gyroscope drift over time
# - Proven effective: Zhang et al. (2024) achieved 97.13% accuracy
# - More interpretable than raw gyroscope (angles vs rad/s)
#
# Architecture:
# - Auto-tuned for 7 channels: 4 heads, 3 layers, 96 dim
# - Deeper network (3 layers) to learn orientation-temporal patterns
#
# References:
#   Madgwick, S. (2010). "An efficient orientation filter for inertial and
#   inertial/magnetic sensor arrays." University of Bristol Tech Report.
#   https://www.x-io.co.uk/open-source-imu-and-ahrs-algorithms/
#
#   Zhang et al. (2024). "Human Activity Recognition Based on Deep Learning
#   Regardless of Sensor Orientation." Applied Sciences, 14(9), 3637.
#   DOI: 10.3390/app14093637 (Achieved 97.13% accuracy using Madgwick+ResNet)

model: Models.imu_transformer.IMUTransformer
dataset: smartfallmm

# Leave-One-Subject-Out (LOSO) setup matching baseline
subjects: [29,30,31,32,34,35,36,37,38,39,43,44,45,46,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63]
validation_subjects: [38, 44]  # Subjects [38, 44]: ~60% ADLs for both acc-only and acc+gyro
# Subjects with poor gyroscope data - permanently in training, never tested
train_only_subjects: [29, 30, 32, 35, 39, 59]  # Subjects with poor gyro data or imbalanced class distribution

model_args:
  # Auto-tuned for 7-channel input (acc + orientation from fusion)
  # Expected: 4 heads, 3 layers, 96 dim (deeper for orientation learning)
  imu_channels: 7              # smv, ax, ay, az, roll, pitch, yaw
  acc_coords: 7                # Alias for backward compatibility
  num_classes: 1               # Binary fall detection
  imu_frames: 128
  acc_frames: 128
  mocap_frames: 128
  dropout: 0.5                 # Matching TransModel baseline
  auto_tune: True              # Enable channel-based auto-tuning
  # Architecture will be auto-selected: num_heads=4, num_layers=3, embed_dim=96

dataset_args:
  mode: 'sliding_window'
  max_length: 128
  task: 'fd'
  modalities: ['accelerometer', 'gyroscope']  # Both needed for fusion
  age_group: ['young']
  sensors: ['watch']
  use_skeleton: False

  # Standard preprocessing (matching baseline)
  enable_filtering: False
  filter_cutoff: 5.5
  filter_fs: 25
  enable_normalization: False
  discard_mismatched_modalities: True  # Drop trials where acc/gyro lengths differ
  length_sensitive_modalities: ['accelerometer', 'gyroscope']

  # Motion filtering (disabled for this experiment)
  enable_motion_filtering: False

  # Quality filtering (disabled - fusion works with all quality levels)
  # Madgwick filter naturally attenuates gyro noise via gradient descent
  enable_gyro_quality_check: False
  quality_mode: 'none'

  # Sensor fusion (ENABLED - Madgwick AHRS)
  enable_sensor_fusion: True
  fusion_method: 'madgwick'       # Madgwick AHRS algorithm
  fusion_frequency: 30.0          # Android IMU sampling rate (Hz)
  fusion_params:
    madgwick_beta: 0.1            # Algorithm gain (convergence rate)
                                   # Higher = faster convergence but more noise
                                   # Lower = smoother but slower response
                                   # 0.1 is optimal for 30Hz sampling

# Training configuration (matching baseline for fair comparison)
batch_size: 64
test_batch_size: 64
val_batch_size: 64
num_epoch: 80

# DataLoader
feeder: Feeder.Make_Dataset.UTD_mm

train_feeder_args:
  batch_size: 64

val_feeder_args:
  batch_size: 64

test_feeder_args:
  batch_size: 64

# Reproducibility + Optimizer (matching baseline)
seed: 2
optimizer: adamw
base_lr: 1e-3
weight_decay: 1e-3

# Expected behavior:
# - Gyroscope data fused into orientation (roll, pitch, yaw) via Madgwick
# - Output: 7-channel input [smv, ax, ay, az, roll°, pitch°, yaw°]
# - Deeper model (3 layers) to learn orientation-temporal patterns
# - Should outperform raw gyro by correcting drift and providing pose context
#
# Madgwick AHRS algorithm overview:
# 1. Predict orientation from gyroscope integration (angular velocity → angles)
# 2. Correct prediction using accelerometer gradient descent (gravity reference)
# 3. Output quaternion converted to Euler angles (roll, pitch, yaw)
#
# Why orientation helps for fall detection:
# - Falls involve rapid changes in device orientation (e.g., wrist rotation)
# - Orientation angles are more stable than raw gyroscope (drift-corrected)
# - Roll/pitch capture tilt relative to gravity (important for fall dynamics)
# - Yaw captures heading changes (less relevant but completes pose)
#
# Comparison with other configs:
# - vs Raw (6ch): Drift correction + orientation context
# - vs Acc-only (4ch): Additional pose information
# - vs Quality hard/adaptive: Works with all samples (no filtering needed)
