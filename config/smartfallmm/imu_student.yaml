# IMU Student Model Configuration - OPTIMIZED FOR ~1700 SAMPLES
# Combined Accelerometer + Gyroscope for Fall Detection
# Ultra-lightweight architecture to prevent overfitting with limited data
#
# Key optimizations:
# - Reduced model size: 1,377 parameters (was 27,425) → 1.23 samples/param
# - Strong regularization: high dropout (0.7) + weight decay (0.02)
# - Smaller batch size for more gradient updates per epoch
# - Learning rate scheduler for better convergence

model: Models.imu_transformer.IMUTransformer
dataset: smartfallmm

# Subjects for fall detection (young participants only)
# All available subjects from data/young/
subjects: [29,30,31,32,34,35,36,37,38,39,43,44,45,46,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63]

# Validation subjects (held out from training to prevent data leakage)
validation_subjects: [38, 44]
# Subjects with poor gyroscope data - permanently in training, never tested (consistency across all configs)
train_only_subjects: [29, 30, 32, 35, 39, 59]  # Subjects with poor gyro data or imbalanced class distribution

model_args:
  # OPTIMIZED ARCHITECTURE - 1,377 parameters for ~1700 samples
  num_layers: 2          # Single layer to minimize parameters
  norm_first: True
  embed_dim: 16           # Minimal embedding (was 32)
  dim_feedforward: 64     # Match embed_dim for minimum params (was 64)
  activation: relu
  imu_channels: 6        # 6 channels: ax, ay, az, gx, gy, gz
  acc_coords: 6          # Alias for backward compatibility
  num_classes: 1         # Binary fall detection (fall vs non-fall)
  imu_frames: 128        # Window size for real-time sliding window
  acc_frames: 128        # Alias for backward compatibility
  mocap_frames: 128      # For compatibility with multimodal code
  num_heads: 1           # Single attention head (was 2)
  dropout: 0.7           # High dropout for strong regularization (was 0.6)

dataset_args:
  mode: 'sliding_window'
  max_length: 128
  task: 'fd'             # Fall detection task
  # IMPORTANT: Include both accelerometer and gyroscope modalities
  modalities: ['skeleton', 'accelerometer', 'gyroscope']
  age_group: ['young']   # Young participants only
  # Change the sensor to phone, watch, meta_wrist, or meta_hip for experiments
  sensors: ['watch']
  use_skeleton: False    # Disable skeleton usage for IMU-only experiments
  # Preprocessing configuration
  enable_filtering: False    # Optional Butterworth low-pass filter
  filter_cutoff: 5.5         # Cutoff frequency in Hz (used only when filtering enabled)
  filter_fs: 25              # Sampling rate in Hz (used only when filtering enabled)
  enable_normalization: False # Toggle per-window standardisation
  discard_mismatched_modalities: True  # Drop trials where acc/gyro lengths differ
  length_sensitive_modalities: ['accelerometer', 'gyroscope']
  # Debugging configuration
  debug: False               # Enable detailed debug logs (trial skipping, all subject issues)

# OPTIMIZED TRAINING CONFIGURATION
# Smaller batches = more updates per epoch (important for small datasets)
# With ~1700 samples: batch_size=32 → ~53 updates/epoch (vs 26 with batch_size=64)
batch_size: 32         # Reduced for more gradient updates (was 64)
test_batch_size: 64
val_batch_size: 64
num_epoch: 100         # More epochs with smaller model + early stopping (was 80)

# Dataloader
feeder: Feeder.Make_Dataset.UTD_mm

# Feeder arguments
train_feeder_args:
  batch_size: 32       # Match training batch size

val_feeder_args:
  batch_size: 64

test_feeder_args:
  batch_size: 64

# Random seed for reproducibility
seed: 2

# OPTIMIZED OPTIMIZER CONFIGURATION
optimizer: adamw
base_lr: 5e-4          # Lower learning rate for stability (was 1e-3)
weight_decay: 0.02     # Strong L2 regularization for small dataset (was 1e-3)

# Learning rate scheduler (recommended: add to main.py if not present)
# step_lr: [60, 80]    # Reduce LR at epochs 60 and 80
# lr_decay_rate: 0.5   # Multiply LR by 0.5 at each step

# OPTIMIZATION SUMMARY FOR ~1700 SAMPLES
# ==========================================
# Model Capacity:
#   - Parameters: 1,377 (reduced from 27,425 = 95% reduction!)
#   - Samples per parameter: 1.23 (was 0.06)
#   - Architecture: 1-layer transformer with minimal embedding
#
# Regularization Strategy:
#   - Dropout: 0.7 (high)
#   - Weight decay: 0.02 (strong L2)
#   - Batch size: 32 (more updates per epoch)
#   - Learning rate: 5e-4 (conservative)
#
# Expected Benefits:
#   - Much better generalization on test set
#   - Reduced overfitting (gap between train/val accuracy)
#   - Faster training (fewer parameters)
#   - Still sufficient model capacity for fall detection task
#
# Alternative Configurations (if needed):
#   - Even safer: embed_dim=6, dim_feedforward=6 → ~1000 params
#   - More capacity: embed_dim=10, dim_feedforward=10 → ~1961 params (0.87 S/P)
#
# Notes:
# - This configuration uses both accelerometer and gyroscope data
# - Ultra-lightweight model optimized for limited training data
# - Filtering is optional (enable_filtering: False by default)
# - Only young participants are used for training
# - Suitable for real-time inference on Android with 128-sample sliding window
